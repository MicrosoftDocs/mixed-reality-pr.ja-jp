---
title: CVPR 2019 で Mixed Reality ヘッドセットワークショップの Computer Vision アプリケーション
description: 2019年6月に CVPR カンファレンスで配信される、Mixed Reality ヘッドセットワークショップの Computer Vision アプリケーションの概要とスケジュール。
author: fbogo
ms.author: febogo
ms.date: 1/9/2019
ms.topic: article
keywords: イベント, 研究モード, cvpr, コンピュータービジョン, 研究, HoloLens
ms.openlocfilehash: 55fbeea1f1293c7df5eae489b6504851bf6bca7f
ms.sourcegitcommit: 09599b4034be825e4536eeb9566968afd021d5f3
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 10/03/2020
ms.locfileid: "91687111"
---
# <a name="computer-vision-applications-for-mixed-reality-headsets"></a><span data-ttu-id="7e596-104">Mixed Reality ヘッドセット用のアプリケーションの Computer Vision</span><span class="sxs-lookup"><span data-stu-id="7e596-104">Computer Vision Applications for Mixed Reality Headsets</span></span>

<span data-ttu-id="7e596-105">[Cvpr 2019](https://cvpr2019.thecvf.com/)と共に構成されます。</span><span class="sxs-lookup"><span data-stu-id="7e596-105">Organized in conjunction with [CVPR 2019](https://cvpr2019.thecvf.com/)</span></span>

<span data-ttu-id="7e596-106">長いビーチ (CA)</span><span class="sxs-lookup"><span data-stu-id="7e596-106">Long Beach (CA)</span></span>

<span data-ttu-id="7e596-107">2019年6月17日 (午後)-Hyatt Regency F</span><span class="sxs-lookup"><span data-stu-id="7e596-107">June 17, 2019 (Afternoon) - Hyatt Regency F</span></span>


## <a name="organizers"></a><span data-ttu-id="7e596-108">若干</span><span class="sxs-lookup"><span data-stu-id="7e596-108">Organizers</span></span>
* <span data-ttu-id="7e596-109">Marc Pollefeys</span><span class="sxs-lookup"><span data-stu-id="7e596-109">Marc Pollefeys</span></span>
* <span data-ttu-id="7e596-110">Federica Bogo</span><span class="sxs-lookup"><span data-stu-id="7e596-110">Federica Bogo</span></span>
* <span data-ttu-id="7e596-111">Johannes Schönberger</span><span class="sxs-lookup"><span data-stu-id="7e596-111">Johannes Schönberger</span></span>
* <span data-ttu-id="7e596-112">Osman Ulusoy</span><span class="sxs-lookup"><span data-stu-id="7e596-112">Osman Ulusoy</span></span>

## <a name="overview"></a><span data-ttu-id="7e596-113">概要</span><span class="sxs-lookup"><span data-stu-id="7e596-113">Overview</span></span>

![周回イメージ](images/cvpr2019_teaser2.jpg)

<span data-ttu-id="7e596-115">Microsoft HoloLens などの Mixed reality ヘッドセットは、コンピュータービジョンアプリケーションを開発するための強力なプラットフォームになりつつあります。</span><span class="sxs-lookup"><span data-stu-id="7e596-115">Mixed reality headsets such as the Microsoft HoloLens are becoming powerful platforms to develop computer vision applications.</span></span> <span data-ttu-id="7e596-116">HoloLens Research モードでは、深度や IR など、すべての未加工のイメージセンサーストリームへのアクセスを提供することにより、デバイス上でコンピュータービジョンを調査できます。</span><span class="sxs-lookup"><span data-stu-id="7e596-116">HoloLens Research Mode enables computer vision research on device by providing access to all raw image sensor streams -- including depth and IR.</span></span> <span data-ttu-id="7e596-117">2018年5月以降に研究モードが利用できるようになったため、いくつかの興味深いデモとアプリケーションが HoloLens 向けに開発されています。</span><span class="sxs-lookup"><span data-stu-id="7e596-117">As Research Mode is now available since May 2018, we are starting to see several interesting demos and applications being developed for HoloLens.</span></span> 

<span data-ttu-id="7e596-118">このワークショップの目的は、混合現実アプリケーションのコンピュータービジョンに関心のある学生と研究者をまとめていくことです。</span><span class="sxs-lookup"><span data-stu-id="7e596-118">The goal of this workshop is to bring together students and researchers interested in computer vision for mixed reality applications.</span></span> <span data-ttu-id="7e596-119">ワークショップは、デモとアプリケーションを共有する会場を提供し、相互に学習して、アプリケーションをビルドしたり、アプリケーションを混合現実に移植したりします。</span><span class="sxs-lookup"><span data-stu-id="7e596-119">The workshop will provide a venue to share demos and applications, and learn from each other to build or port applications to mixed reality.</span></span> 

<span data-ttu-id="7e596-120">マイクロソフトは、(ego 中心の) オブジェクト認識、手作業とユーザー追跡、アクティビティ認識、酷評、3D 再構築、シーンの理解、センサーベースのローカリゼーション、ナビゲーションなどのトピックに対する送信を推奨しています。</span><span class="sxs-lookup"><span data-stu-id="7e596-120">We encourage submissions on the topics of (ego-centric) object recognition, hand and user tracking, activity recognition, SLAM, 3D reconstruction, scene understanding, sensor-based localization, navigation and more.</span></span>

## <a name="paper-submission"></a><span data-ttu-id="7e596-121">紙の送信</span><span class="sxs-lookup"><span data-stu-id="7e596-121">Paper Submission</span></span>
* <span data-ttu-id="7e596-122">用紙送信期限: 5 月17日</span><span class="sxs-lookup"><span data-stu-id="7e596-122">Paper submission deadline: May 17</span></span>
* <span data-ttu-id="7e596-123">作成者への通知: 5 月24日</span><span class="sxs-lookup"><span data-stu-id="7e596-123">Notification to authors: May 24</span></span>

<span data-ttu-id="7e596-124">紙の送信では、CVPR テンプレートを使用する必要があり、4ページと参照に制限されています。</span><span class="sxs-lookup"><span data-stu-id="7e596-124">Paper submissions should use the CVPR template and are limited to 4 pages plus references.</span></span> <span data-ttu-id="7e596-125">また、作成者は、アプリケーションを紹介するビデオを送信することをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="7e596-125">In addition, we encourage the authors to submit a video showcasing their application.</span></span>
<span data-ttu-id="7e596-126">以前に発行された作業の送信が許可されていることに注意してください (メインの CVPR 2019 カンファレンスに受け入れられた作業を含む)。</span><span class="sxs-lookup"><span data-stu-id="7e596-126">Note that submissions of previously published work are allowed (including work accepted to the main CVPR 2019 conference).</span></span> 

<span data-ttu-id="7e596-127">送信は CMT にアップロードできます。 https://cmt3.research.microsoft.com/CVFORMR2019</span><span class="sxs-lookup"><span data-stu-id="7e596-127">Submissions can be uploaded to the CMT: https://cmt3.research.microsoft.com/CVFORMR2019</span></span>

<span data-ttu-id="7e596-128">ワークショップでは、口頭プレゼンテーション用に用紙のサブセットが選択されます。</span><span class="sxs-lookup"><span data-stu-id="7e596-128">A subset of papers will be selected for oral presentation at the workshop.</span></span> <span data-ttu-id="7e596-129">ただし、すべての作成者がデモセッション中に作業を提示することを強くお勧めします。</span><span class="sxs-lookup"><span data-stu-id="7e596-129">However, we strongly encourage all the authors to present their work during the demo session.</span></span>


## <a name="schedule"></a><span data-ttu-id="7e596-130">スケジュール</span><span class="sxs-lookup"><span data-stu-id="7e596-130">Schedule</span></span>
* <span data-ttu-id="7e596-131">13:30-13:45: 開始して解説を開始しています。</span><span class="sxs-lookup"><span data-stu-id="7e596-131">13:30-13:45: Welcome and Opening Remarks.</span></span>
* <span data-ttu-id="7e596-132">13:45-14:15: **基調講演** : 収益性 Marc Pollefeys, ETH チューリヒ/Microsoft.</span><span class="sxs-lookup"><span data-stu-id="7e596-132">13:45-14:15: **Keynote talk** : Prof. Marc Pollefeys, ETH Zurich/Microsoft.</span></span> <span data-ttu-id="7e596-133">Title: HoloLens の Egocentric Computer Vision。</span><span class="sxs-lookup"><span data-stu-id="7e596-133">Title: Egocentric Computer Vision on HoloLens.</span></span>
* <span data-ttu-id="7e596-134">14:15-14:45: **基調講演** : 収益性 Kris Kitani、カーネギーメロン大学。</span><span class="sxs-lookup"><span data-stu-id="7e596-134">14:15-14:45: **Keynote talk** : Prof. Kris Kitani, Carnegie Mellon University.</span></span> <span data-ttu-id="7e596-135">Title: Egocentric アクティビティを実行し、予測を行います。</span><span class="sxs-lookup"><span data-stu-id="7e596-135">Title: Egocentric Activity and Pose Forecasting.</span></span>
* <span data-ttu-id="7e596-136">14:45-15:15: **基調講演** : Yang liu、カリフォルニア国立テクノロジ。</span><span class="sxs-lookup"><span data-stu-id="7e596-136">14:45-15:15: **Keynote talk** : Dr. Yang Liu, California Institute of Technology.</span></span> <span data-ttu-id="7e596-137">タイトル: 補強された現実を備えた視覚障碍アシスタントのパワー</span><span class="sxs-lookup"><span data-stu-id="7e596-137">Title: Powering a Cognitive Assistant for the Blind with Augmented Reality.</span></span>
* <span data-ttu-id="7e596-138">15:15-16:15: コーヒーの休憩とデモ。</span><span class="sxs-lookup"><span data-stu-id="7e596-138">15:15-16:15: Coffee break and demos.</span></span>
* <span data-ttu-id="7e596-139">16:15-16:45: **基調講演** : 収益性 Kristen Grauman、テキサス州オースティン/Facebook AI Research。</span><span class="sxs-lookup"><span data-stu-id="7e596-139">16:15-16:45: **Keynote talk** : Prof. Kristen Grauman, University of Texas at Austin/Facebook AI Research.</span></span> <span data-ttu-id="7e596-140">Title: ファーストユーザーのビデオでの人間とオブジェクトのやり取り。</span><span class="sxs-lookup"><span data-stu-id="7e596-140">Title: Human-object interaction in first-person video.</span></span>
* <span data-ttu-id="7e596-141">16:45-17:15: 口頭プレゼンテーション:</span><span class="sxs-lookup"><span data-stu-id="7e596-141">16:45-17:15: Oral presentations:</span></span>
    * <span data-ttu-id="7e596-142">登録により、HoloLens を使用したスタンドアロンの orthopedic ナビゲーションが容易になりました。</span><span class="sxs-lookup"><span data-stu-id="7e596-142">Registration made easy - standalone orthopedic navigation with HoloLens.</span></span> <span data-ttu-id="7e596-143">F.</span><span class="sxs-lookup"><span data-stu-id="7e596-143">F.</span></span> <span data-ttu-id="7e596-144">Liebmann、Roner、Atzigen、Wanivenhaus、、Neuhaus、、、Spirig、。 Scaramuzza、Sutter、Snedeker、Farshad、Furnstahl、、のようになります。</span><span class="sxs-lookup"><span data-stu-id="7e596-144">Liebmann, S. Roner, M. von Atzigen, F. Wanivenhaus, C. Neuhaus, J. Spirig, D. Scaramuzza, R. Sutter, J. Snedeker, M. Farshad, P. Furnstahl.</span></span>
    * <span data-ttu-id="7e596-145">HoloLens を使用して、ステレオを学習します。</span><span class="sxs-lookup"><span data-stu-id="7e596-145">Learning stereo by walking around with a HoloLens.</span></span> <span data-ttu-id="7e596-146">H.</span><span class="sxs-lookup"><span data-stu-id="7e596-146">H.</span></span> <span data-ttu-id="7e596-147">Zhan、Pekelny、Ulusoy。</span><span class="sxs-lookup"><span data-stu-id="7e596-147">Zhan, Y. Pekelny, O. Ulusoy.</span></span>
* <span data-ttu-id="7e596-148">17:15-17:30: 最後の注釈です。</span><span class="sxs-lookup"><span data-stu-id="7e596-148">17:15-17:30: Final Remarks.</span></span>
