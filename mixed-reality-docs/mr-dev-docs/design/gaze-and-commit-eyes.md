---
title: 目の視線入力とコミット
description: 目の視線入力とコミットの入力モデルについて説明します。
author: sostel
ms.author: sostel
ms.date: 05/05/2019
ms.topic: article
ms.localizationpriority: high
keywords: 視線追跡, Mixed Reality, 入力, 目の視線入力, 目のターゲット設定, HoloLens 2, 視線に基づく選択, Mixed Reality ヘッドセット, Windows Mixed Reality ヘッドセット, 仮想現実ヘッドセット, HoloLens, MRTK, Mixed Reality Toolkit, 視線入力
ms.openlocfilehash: 1f337d3cbc1f82b4f69194d4b903687be067f9d6
ms.sourcegitcommit: d340303cda71c31e6c3320231473d623c0930d33
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 01/01/2021
ms.locfileid: "97847871"
---
# <a name="eye-gaze-and-commit"></a>目の視線入力とコミット

_目の視線入力とコミット_ は、オブジェクトを見ることによってターゲットにすることが含まれる、[視線入力とコミット](gaze-and-commit.md)入力モデルの特殊なケースです。 ユーザーは、手のジェスチャ、音声コマンド、ゲーム コントローラーのような周辺入力機器など、セカンダリ "_コミット_" 入力を使用してそのターゲットを操作できます。 

HoloLens 2 では、頭の視線入力ではなく目の視線入力を使用することで _視線入力とコミット_ をより迅速で快適なものにできます。 一般的な[頭の視線入力とコミット](gaze-and-commit.md)の相互作用モデルを拡張するには: 
1. ターゲットを見る。 
2. 次のような明示的セカンダリ入力を使用して、そのターゲットを選択するという意図を確認する。  
   - 手のジェスチャ (エアタップなど)
   - ボタン押下 (Bluetooth キーボードやクリッカーなど)
   - 音声コマンド ("選択" など)
   - ドウェル (ユーザーがターゲットを見つめ続けて選択する方法)

ただし、目の視線入力はいくつかの点で頭の視線入力と動作が異なっており、さまざまな固有の課題があります。 ホログラフィック アプリで視線追跡を入力として使用する場合の一般的なメリットと課題の概要については、[目の視線入力の設計ガイドライン](eye-tracking.md)に関するページを参照してください。 このセクションでは、_目の視線入力とコミット_ に関する特定の設計考慮事項を中心に説明します。
まず、目は非常に速く動かせるので、視界からすばやくターゲット設定を行うのに適しています。 目の視線入力は、特に、エアタップやボタン押下などの高速コミットと組み合わせると、視線入力とコミットをすばやく行うのに最適です。
   
## <a name="design-guidelines-for-eye-gaze-and-commit"></a>目の視線入力とコミットに関する設計ガイドライン

**カーソルを表示しない**:頭の視線入力を使用する場合、カーソルを使わずに操作するのはほぼ不可能ですが、目の視線入力を使用する場合は、カーソルが表示されると気が散って目障りに感じられます。 視線追跡が機能しているかどうかや、現在見ているターゲットを正しく検出したかどうかをユーザーに知らせるには、カーソルに頼るのではなく、繊細なビジュアル ハイライトを使用します。

**繊細なブレンド ホバー フィードバックを追求する**:頭の視線入力にとっては優れた視覚的フィードバックが、目の視線入力にとっては不快で混乱するエクスペリエンスになる可能性があります。 目の動きは非常に速いため、視野の中の点から点へとすばやく動くことにご注意ください。 周りを見回しているときに急にハイライトが変化 (オン/オフ) すると、フィードバックがちらつく可能性があります。 そのため、ホバー フィードバックを提供する場合は、スムーズ ブレンドイン ハイライト (視線を外す場合はスムーズ ブレンドアウト ハイライト) を使用することをお勧めします。 このようにすると、ターゲットを見たときに、最初はフィードバックにほとんど気付きません。 500 - 1000 ミリ秒かけて、ハイライトの輝度が上がります。 初級ユーザーは見つめているターゲットが正しく判別されたことを確かめようとしてターゲットを見つめ続けるかもしれませんが、上級ユーザーはフィードバックが最大輝度になるのを待たずにすばやく視線入力とコミットを実行できます。 また、ホバー フィードバックをフェードアウトするときにブレンドアウトを使用することもお勧めします。 調査により、周辺視野 (自分が注視していない視野領域) では動きやコントラストの急激な変化に気付きやすいことがわかっています。
フェードアウトをブレンドインと同じほど遅くする必要はありません。 このことは、ハイライトのコントラストが高い場合や色が変化する場合にのみ重要です。 最初のうちホバー フィードバックが繊細な場合、違いに気付かないかもしれません。

**視線入力とコミット信号を同期させるよう配慮する**:入力信号の同期は、単純なエア タップやボタン押下の場合は大きな問題ではありません。 長い音声コマンドや複雑な手のジェスチャが必要になる可能性のある、より複雑なコミット アクションを使用する場合は注意が必要です。 ターゲットを見つめて、長い音声コマンドを発音する場合を想像してください。 発音に必要な時間と、発音された内容をシステムが検出するのに必要な時間を考えると、目の視線入力はシーン内の離れた場所にある新しいターゲットに移動している可能性があります。 そのため、コマンドが認識されるまでターゲットを見つめ続けなければならないことをユーザーに気付かせるか、コマンドの開始とユーザーがその時点で見ていたものを特定するという方法で入力を処理する必要があります。

## <a name="see-also"></a>関連項目

* [視線ベースの操作] (eye-gaze-interaction.md)
* [HoloLens 2 上の視線追跡] (eye-tracking.md)
* [視線入力とコミット](gaze-and-commit.md)
* [視線入力とドウェル](gaze-and-dwell.md)
* [手 - 直接操作](direct-manipulation.md)
* [手 - ジェスチャ](gaze-and-commit.md#composite-gestures)
* [手 - ポイントとコミット](point-and-commit.md)
* [本能的な操作](interaction-fundamentals.md)
* [音声入力](voice-input.md)
